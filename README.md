
## ğŸš€ Overview

Touchless Interaction with Kiosk is a computer vision-based system that enables users to interact with a kiosk interface using hand gestures instead of physical touch.

The project was developed to improve hygiene, accessibility, and user experience in public environments such as ATMs, ticketing machines, and self-service kiosks.

---

## ğŸ¯ Problem Statement

Traditional kiosks require physical contact, which can:

* Spread germs in public environments
* Create accessibility challenges
* Reduce user convenience

This project provides a **contactless interaction system** using real-time hand gesture recognition.

---

## ğŸ› ï¸ Technologies Used

* Flask â€“ Web framework
* OpenCV â€“ Real-time computer vision
* TensorFlow â€“ Deep learning framework for gesture model
* MediaPipe â€“ Hand tracking and gesture recognition
* HTML/CSS â€“ Frontend interface
* Pretrained model: gesture_model.h5

---

## âœ¨ Features

* Real-time hand tracking
* Gesture-based menu navigation
* Touch-free interaction
* Lightweight and efficient processing
* Achieved **>90% gesture detection accuracy** during testing

---

## ğŸ§  How It Works (Technical Explanation)

The system uses real-time frame processing through OpenCV.

* Frames are captured from the webcam
* Image preprocessing (grayscale conversion, thresholding, contour detection) is applied
* Hand region is extracted
* Gesture patterns are classified based on shape and movement
* Detected gestures trigger corresponding UI events

The system is optimized to reduce latency and ensure smooth user experience.


---

## ğŸ–¥ï¸ Installation & Setup

```bash
# Clone the repository
git clone https://github.com/your-username/touchless-kiosk.git

# Create a virtual environment:
python -m venv venv
venv\Scripts\activate      # For Windows
# OR
source venv/bin/activate   # For macOS/Linux

# Install dependencies:
pip install -r requirements.txt

# Run the project
python main.py
```

---

## ğŸ“‚  Folder Structure

```
Touchless_Interaction_With_Kisok/
â”‚
â”œâ”€â”€ app.py                      # Main backend script
â”œâ”€â”€ gesture_model.h5            # Trained gesture model
â”œâ”€â”€ index111.html               # HTML UI
â”‚
â”œâ”€â”€ static/                     # Assets (images, styles)
â”‚   â”œâ”€â”€ *.png                   # Icon images
â”‚   â”œâ”€â”€ *.jpg                   # Background/check-in images
â”‚   â”œâ”€â”€ styles.css              # Styling
â”‚   â”œâ”€â”€ styles1.css
â”‚   â””â”€â”€ styles2.css

```

---

##ğŸ“¦ requirements.txt

```
Flask
opencv-python
tensorflow
mediapipe
numpy

```

---

## âš ï¸ Challenges Faced

* Handling varying lighting conditions
* Background noise interference
* Reducing detection delay
* Improving gesture accuracy

---

## ğŸ“ˆ Results

* Achieved more than 90% gesture detection accuracy
* Successfully integrated gesture recognition with kiosk interface
* Improved accessibility and hygiene in public interaction systems

